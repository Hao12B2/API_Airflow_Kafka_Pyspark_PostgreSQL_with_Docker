[2025-03-06T14:13:23.900+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T14:13:23.924+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T14:13:23.934+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T14:13:23.935+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T14:13:23.951+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T14:13:23.959+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpcyuxt3fv']
[2025-03-06T14:13:23.963+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask start
[2025-03-06T14:13:23.963+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=330) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T14:13:23.964+0000] {standard_task_runner.py:72} INFO - Started process 332 to run task
[2025-03-06T14:13:24.036+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host 13fb702d3c09
[2025-03-06T14:13:24.147+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T14:13:24.148+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T14:13:24.224+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T14:13:24.225+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T14:13:24.243+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T14:13:24.243+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T141323, end_date=20250306T141324
[2025-03-06T14:13:24.300+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T14:13:24.344+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T14:13:24.346+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T14:41:44.868+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T14:41:44.895+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T14:41:44.904+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T14:41:44.905+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T14:41:44.926+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T14:41:44.936+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpv3_bb1kt']
[2025-03-06T14:41:44.940+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask start
[2025-03-06T14:41:44.942+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=255) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T14:41:44.943+0000] {standard_task_runner.py:72} INFO - Started process 258 to run task
[2025-03-06T14:41:45.011+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host a38ba10c8f5b
[2025-03-06T14:41:45.123+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T14:41:45.124+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T14:41:45.153+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T14:41:45.153+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T14:41:45.166+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T14:41:45.167+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T144144, end_date=20250306T144145
[2025-03-06T14:41:45.238+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T14:41:45.276+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T14:41:45.279+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T15:15:58.081+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T15:15:58.104+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T15:15:58.112+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T15:15:58.113+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T15:15:58.131+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T15:15:58.139+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpv4254syr']
[2025-03-06T15:15:58.142+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask start
[2025-03-06T15:15:58.143+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=260) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T15:15:58.144+0000] {standard_task_runner.py:72} INFO - Started process 261 to run task
[2025-03-06T15:15:58.206+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host e9dfa152b6ab
[2025-03-06T15:15:58.306+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T15:15:58.308+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T15:15:58.341+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T15:15:58.342+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T15:15:58.354+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T15:15:58.355+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T151558, end_date=20250306T151558
[2025-03-06T15:15:58.399+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T15:15:58.433+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T15:15:58.436+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T15:39:11.487+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T15:39:11.525+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T15:39:11.540+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T15:39:11.541+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T15:39:11.581+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T15:39:11.589+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmp063sy48n']
[2025-03-06T15:39:11.592+0000] {standard_task_runner.py:105} INFO - Job 26: Subtask start
[2025-03-06T15:39:11.594+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=1985) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T15:39:11.597+0000] {standard_task_runner.py:72} INFO - Started process 1987 to run task
[2025-03-06T15:39:11.696+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host e9dfa152b6ab
[2025-03-06T15:39:11.799+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T15:39:11.800+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T15:39:11.817+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T15:39:11.817+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T15:39:11.832+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T15:39:11.833+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T153911, end_date=20250306T153911
[2025-03-06T15:39:11.974+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T15:39:12.013+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T15:39:12.028+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:12:06.013+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:12:06.034+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:12:06.042+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:12:06.043+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:12:06.059+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:12:06.066+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpx4y4y_ya']
[2025-03-06T16:12:06.069+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask start
[2025-03-06T16:12:06.071+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=226) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:12:06.072+0000] {standard_task_runner.py:72} INFO - Started process 228 to run task
[2025-03-06T16:12:06.134+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host 6dfac24f2f32
[2025-03-06T16:12:06.242+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:12:06.243+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:12:06.293+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:12:06.293+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:12:06.307+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:12:06.308+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T161206, end_date=20250306T161206
[2025-03-06T16:12:06.367+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:12:06.404+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:12:06.409+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:13:51.923+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:13:51.946+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:13:51.955+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:13:51.956+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:13:51.972+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:13:51.980+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpjqdrxgl_']
[2025-03-06T16:13:51.983+0000] {standard_task_runner.py:105} INFO - Job 12: Subtask start
[2025-03-06T16:13:51.984+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=802) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:13:51.985+0000] {standard_task_runner.py:72} INFO - Started process 804 to run task
[2025-03-06T16:13:52.044+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host 6dfac24f2f32
[2025-03-06T16:13:52.140+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:13:52.141+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:13:52.157+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:13:52.157+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:13:52.170+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:13:52.171+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T161351, end_date=20250306T161352
[2025-03-06T16:13:52.241+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:13:52.275+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:13:52.278+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:26:37.020+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:26:37.042+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:26:37.050+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:26:37.051+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:26:37.068+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:26:37.075+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpwc_xbf30']
[2025-03-06T16:26:37.078+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask start
[2025-03-06T16:26:37.079+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=215) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:26:37.080+0000] {standard_task_runner.py:72} INFO - Started process 216 to run task
[2025-03-06T16:26:37.139+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host bfcfbd2155da
[2025-03-06T16:26:37.238+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:26:37.239+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:26:37.271+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:26:37.271+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:26:37.284+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:26:37.285+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T162637, end_date=20250306T162637
[2025-03-06T16:26:37.336+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:26:37.369+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:26:37.374+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:29:14.657+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:29:14.680+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:29:14.689+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:29:14.689+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:29:14.705+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:29:14.713+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpm2m6m8do']
[2025-03-06T16:29:14.716+0000] {standard_task_runner.py:105} INFO - Job 14: Subtask start
[2025-03-06T16:29:14.716+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=866) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:29:14.717+0000] {standard_task_runner.py:72} INFO - Started process 869 to run task
[2025-03-06T16:29:14.779+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host bfcfbd2155da
[2025-03-06T16:29:14.878+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:29:14.879+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:29:14.895+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:29:14.896+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:29:14.909+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:29:14.910+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T162914, end_date=20250306T162914
[2025-03-06T16:29:14.973+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:29:15.010+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:29:15.013+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:37:19.974+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:37:20.000+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:37:20.011+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:37:20.012+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:37:20.031+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:37:20.040+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpgn23p7hi']
[2025-03-06T16:37:20.043+0000] {standard_task_runner.py:105} INFO - Job 30: Subtask start
[2025-03-06T16:37:20.044+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=1995) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:37:20.046+0000] {standard_task_runner.py:72} INFO - Started process 1997 to run task
[2025-03-06T16:37:20.115+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host bfcfbd2155da
[2025-03-06T16:37:20.211+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:37:20.212+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:37:20.228+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:37:20.229+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:37:20.242+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:37:20.243+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T163720, end_date=20250306T163720
[2025-03-06T16:37:20.301+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:37:20.336+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:37:20.340+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T16:39:21.318+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T16:39:21.340+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:39:21.349+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T16:39:21.350+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T16:39:21.367+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T16:39:21.375+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=2464) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T16:39:21.376+0000] {standard_task_runner.py:72} INFO - Started process 2465 to run task
[2025-03-06T16:39:21.375+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmp6h0qdi79']
[2025-03-06T16:39:21.377+0000] {standard_task_runner.py:105} INFO - Job 38: Subtask start
[2025-03-06T16:39:21.439+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host bfcfbd2155da
[2025-03-06T16:39:21.539+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T16:39:21.540+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T16:39:21.556+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T16:39:21.556+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T16:39:21.569+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T16:39:21.570+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T163921, end_date=20250306T163921
[2025-03-06T16:39:21.632+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T16:39:21.671+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T16:39:21.676+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T17:13:46.780+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T17:13:46.808+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T17:13:46.825+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T17:13:46.826+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T17:13:46.849+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T17:13:46.859+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpz4cdd3at']
[2025-03-06T17:13:46.862+0000] {standard_task_runner.py:105} INFO - Job 2: Subtask start
[2025-03-06T17:13:46.863+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=235) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T17:13:46.864+0000] {standard_task_runner.py:72} INFO - Started process 237 to run task
[2025-03-06T17:13:46.940+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host 0bf4ff38bfab
[2025-03-06T17:13:47.075+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T17:13:47.076+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T17:13:47.115+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T17:13:47.115+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T17:13:47.129+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T17:13:47.130+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T171346, end_date=20250306T171347
[2025-03-06T17:13:47.200+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T17:13:47.234+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T17:13:47.238+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-03-06T17:33:35.473+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-03-06T17:33:35.503+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T17:33:35.512+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [queued]>
[2025-03-06T17:33:35.512+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-03-06T17:33:35.529+0000] {taskinstance.py:2890} INFO - Executing <Task(PythonOperator): start> on 2025-03-05 00:00:00+00:00
[2025-03-06T17:33:35.538+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'spark_flow', 'start', 'scheduled__2025-03-05T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/dag_***_spark.py', '--cfg-path', '/tmp/tmpdbnbczvn']
[2025-03-06T17:33:35.541+0000] {standard_task_runner.py:105} INFO - Job 3: Subtask start
[2025-03-06T17:33:35.542+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=255) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-03-06T17:33:35.543+0000] {standard_task_runner.py:72} INFO - Started process 257 to run task
[2025-03-06T17:33:35.604+0000] {task_command.py:467} INFO - Running <TaskInstance: spark_flow.start scheduled__2025-03-05T00:00:00+00:00 [running]> on host e4b312e1a8d1
[2025-03-06T17:33:35.704+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='haonv' AIRFLOW_CTX_DAG_ID='spark_flow' AIRFLOW_CTX_TASK_ID='start' AIRFLOW_CTX_EXECUTION_DATE='2025-03-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-03-05T00:00:00+00:00'
[2025-03-06T17:33:35.705+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-03-06T17:33:35.744+0000] {logging_mixin.py:190} INFO - Jobs started
[2025-03-06T17:33:35.745+0000] {python.py:240} INFO - Done. Returned value was: None
[2025-03-06T17:33:35.760+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-03-06T17:33:35.761+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=spark_flow, task_id=start, run_id=scheduled__2025-03-05T00:00:00+00:00, execution_date=20250305T000000, start_date=20250306T173335, end_date=20250306T173335
[2025-03-06T17:33:35.838+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-03-06T17:33:35.879+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-03-06T17:33:35.883+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
